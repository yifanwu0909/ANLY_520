{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Working with SparkSQL (5 points)\n",
    "\n",
    "This is an interactive PySpark session. Remember that when you open this notebook the `SparkContext` and `SparkSession` are already created, and they are in the `sc` and `spark` variables, respectively. You can run the following two cells to make sure that the Kernel is active.\n",
    "\n",
    "**Do not insert any additional cells than the ones that are provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-95-236.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-95-236.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1538170cd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quazyilx again!\n",
    "\n",
    "Yes, you remember it. As a reminder, here is the description of the files.\n",
    "\n",
    "The quazyilx has been malfunctioning, and occasionally generates output with a `-1` for all four measurements, like this:\n",
    "\n",
    "    2015-12-10T08:40:10Z fnard:-1 fnok:-1 cark:-1 gnuck:-1\n",
    "\n",
    "There are four different versions of the _quazyilx_ file, each of a different size. As you can see in the output below the file sizes are 50MB (1,000,000 rows), 4.8GB (100,000,000 rows), 18GB (369,865,098 rows) and 36.7GB (752,981,134 rows). The only difference is the length of the number of records, the file structure is the same.\n",
    "\n",
    "```\n",
    "[hadoop@ip-172-31-1-240 ~]$ hadoop fs -ls s3://bigdatateaching/quazyilx/\n",
    "Found 4 items\n",
    "-rw-rw-rw-   1 hadoop hadoop    52443735 2018-01-25 15:37 s3://bigdatateaching/quazyilx/quazyilx0.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop  5244417004 2018-01-25 15:37 s3://bigdatateaching/quazyilx/quazyilx1.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop 19397230888 2018-01-25 15:38 s3://bigdatateaching/quazyilx/quazyilx2.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop 39489364082 2018-01-25 15:41 s3://bigdatateaching/quazyilx/quazyilx3.txt\n",
    "```\n",
    "\n",
    "You will use Spark and SparkSQL to create a Spark DataFrame and then run some analysis on the files using SparkSQL queries.\n",
    "\n",
    "First, in the following cell, create an RDD called `quazyilx` that reads the `quazyilx1.txt` file from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quazyilx = sc.textFile(\"s3://bigdatateaching/quazyilx/quazyilx1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, look at the first 50 elements of `quazyilx` to make sure everything is working corectly. This should take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2000-01-01 00:00:03 fnard:7 fnok:8 cark:19 gnuck:25',\n",
       " u'2000-01-01 00:00:08 fnard:14 fnok:19 cark:16 gnuck:37',\n",
       " u'2000-01-01 00:00:17 fnard:12 fnok:11 cark:12 gnuck:8',\n",
       " u'2000-01-01 00:00:22 fnard:18 fnok:16 cark:3 gnuck:8',\n",
       " u'2000-01-01 00:00:32 fnard:7 fnok:16 cark:7 gnuck:37',\n",
       " u'2000-01-01 00:00:40 fnard:6 fnok:14 cark:3 gnuck:30',\n",
       " u'2000-01-01 00:00:47 fnard:11 fnok:10 cark:17 gnuck:7',\n",
       " u'2000-01-01 00:00:55 fnard:9 fnok:14 cark:13 gnuck:30',\n",
       " u'2000-01-01 00:00:56 fnard:10 fnok:1 cark:7 gnuck:6',\n",
       " u'2000-01-01 00:00:59 fnard:11 fnok:11 cark:12 gnuck:18',\n",
       " u'2000-01-01 00:01:03 fnard:9 fnok:13 cark:14 gnuck:49',\n",
       " u'2000-01-01 00:01:06 fnard:12 fnok:10 cark:19 gnuck:30',\n",
       " u'2000-01-01 00:01:16 fnard:0 fnok:12 cark:19 gnuck:26',\n",
       " u'2000-01-01 00:01:26 fnard:10 fnok:11 cark:10 gnuck:49',\n",
       " u'2000-01-01 00:01:30 fnard:9 fnok:5 cark:16 gnuck:13',\n",
       " u'2000-01-01 00:01:38 fnard:11 fnok:10 cark:7 gnuck:47',\n",
       " u'2000-01-01 00:01:43 fnard:2 fnok:2 cark:20 gnuck:35',\n",
       " u'2000-01-01 00:01:53 fnard:12 fnok:11 cark:20 gnuck:3',\n",
       " u'2000-01-01 00:01:54 fnard:6 fnok:6 cark:16 gnuck:18',\n",
       " u'2000-01-01 00:02:01 fnard:9 fnok:10 cark:17 gnuck:15',\n",
       " u'2000-01-01 00:02:08 fnard:0 fnok:6 cark:3 gnuck:49',\n",
       " u'2000-01-01 00:02:10 fnard:15 fnok:12 cark:3 gnuck:14',\n",
       " u'2000-01-01 00:02:17 fnard:7 fnok:-1 cark:18 gnuck:25',\n",
       " u'2000-01-01 00:02:26 fnard:10 fnok:0 cark:4 gnuck:26',\n",
       " u'2000-01-01 00:02:35 fnard:4 fnok:17 cark:14 gnuck:6',\n",
       " u'2000-01-01 00:02:43 fnard:10 fnok:15 cark:19 gnuck:2',\n",
       " u'2000-01-01 00:02:50 fnard:8 fnok:16 cark:7 gnuck:1',\n",
       " u'2000-01-01 00:02:55 fnard:17 fnok:4 cark:17 gnuck:23',\n",
       " u'2000-01-01 00:03:05 fnard:7 fnok:14 cark:18 gnuck:8',\n",
       " u'2000-01-01 00:03:10 fnard:12 fnok:19 cark:7 gnuck:43',\n",
       " u'2000-01-01 00:03:15 fnard:4 fnok:4 cark:4 gnuck:50',\n",
       " u'2000-01-01 00:03:17 fnard:17 fnok:4 cark:13 gnuck:22',\n",
       " u'2000-01-01 00:03:27 fnard:19 fnok:1 cark:18 gnuck:0',\n",
       " u'2000-01-01 00:03:32 fnard:10 fnok:11 cark:4 gnuck:40',\n",
       " u'2000-01-01 00:03:40 fnard:5 fnok:10 cark:6 gnuck:45',\n",
       " u'2000-01-01 00:03:46 fnard:19 fnok:20 cark:7 gnuck:36',\n",
       " u'2000-01-01 00:03:49 fnard:7 fnok:15 cark:17 gnuck:44',\n",
       " u'2000-01-01 00:03:52 fnard:7 fnok:8 cark:17 gnuck:11',\n",
       " u'2000-01-01 00:03:59 fnard:1 fnok:4 cark:11 gnuck:21',\n",
       " u'2000-01-01 00:04:09 fnard:12 fnok:-1 cark:0 gnuck:48',\n",
       " u'2000-01-01 00:04:10 fnard:4 fnok:19 cark:16 gnuck:30',\n",
       " u'2000-01-01 00:04:14 fnard:18 fnok:7 cark:5 gnuck:11',\n",
       " u'2000-01-01 00:04:18 fnard:19 fnok:11 cark:3 gnuck:29',\n",
       " u'2000-01-01 00:04:21 fnard:18 fnok:14 cark:14 gnuck:35',\n",
       " u'2000-01-01 00:04:23 fnard:0 fnok:19 cark:20 gnuck:17',\n",
       " u'2000-01-01 00:04:30 fnard:7 fnok:7 cark:6 gnuck:42',\n",
       " u'2000-01-01 00:04:34 fnard:14 fnok:2 cark:13 gnuck:39',\n",
       " u'2000-01-01 00:04:41 fnard:19 fnok:14 cark:12 gnuck:32',\n",
       " u'2000-01-01 00:04:51 fnard:19 fnok:18 cark:20 gnuck:34',\n",
       " u'2000-01-01 00:04:57 fnard:3 fnok:1 cark:20 gnuck:16']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quazyilx.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to work with the RDD to be able to make a DataFrame. In the following cell, modify the code to create a class called `quazyilx_class` that processes a line and returns it as a slotted structure, with attributes for the `.time`, `.fnard`, `.fnok` and `.cark`. \n",
    "\n",
    "You will need to define the Regular Expression and complete the class where it says `#Put your code here:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os,datetime,re\n",
    "from pyspark.sql import Row\n",
    "\n",
    "QUAZYILX_RE = \"(\\d+\\-\\d+\\-\\d+\\s\\d+\\:\\d+\\:\\d+)\\s(fnard\\:-?\\d+)\\s(fnok\\:-?\\d+)\\s(cark\\:-?\\d+)\\s(gnuck\\:-?\\d+)\"\n",
    "quazyilx_re = re.compile(QUAZYILX_RE)\n",
    "\n",
    "class quazyilx_class():\n",
    "    __slots__ = ['datetime','fnard','fnok','cark','gnuck']\n",
    "    def __init__(self,line):\n",
    "        # Parse line with the regular expression\n",
    "        m = quazyilx_re.search(line)\n",
    "        if not m:\n",
    "            self.datetime = None\n",
    "            self.fnard    = None\n",
    "            self.fnok     = None\n",
    "            self.cark     = None\n",
    "            self.gnuck    = None\n",
    "            return\n",
    "        # Put your code here:\n",
    "        if m:\n",
    "            from datetime import datetime\n",
    "            self.datetime = datetime.strptime(m.group(1), '%Y-%m-%d %H:%M:%S')\n",
    "            self.fnard    = int(m.group(2).split(':')[1])\n",
    "            self.fnok     = int(m.group(3).split(':')[1])\n",
    "            self.cark     = int(m.group(4).split(':')[1])\n",
    "            self.gnuck    = int(m.group(5).split(':')[1])\n",
    "            return \n",
    "                 \n",
    "    def __str__(self):\n",
    "        return \"{} fnard:{} fnok:{} cark:{} gnuck:{}\".format(self.datetime,self.fnard,self.fnok,self.cark,self.gnuck)\n",
    "    def __repr__(self):\n",
    "        return \"{} fnard:{} fnok:{} cark:{} gnuck:{}\".format(self.datetime,self.fnard,self.fnok,self.cark,self.gnuck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then need to turn the quazyilx RDD into a `Row()` object. [Look at the notebook we used in Lab08 for reference](https://github.com/gu-anly502/lab08-spring-2018/blob/master/social-characteristics-of-the-marvel-universe.ipynb). You can do that with a lambda function, like this:\n",
    "\n",
    "```(python)\n",
    "lambda q:Row(datetime=q.datetime.isoformat(),fnard=q.fnard,fnok=q.fnok,cark=q.cark,gnuck=q.gnuck))\n",
    "```\n",
    "\n",
    "Alternatively, you can add a new method to the Quazyilx class called `.Row()` that returns a Row. All of these ways are more or less equivalent. You just need to pick one of them.  You may find it useful to look at [this documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection).\n",
    "\n",
    "In the next cell, create an RDD called `line` that converts the `quazyilx` RDD into a `Row()` object using the `quazyilx_class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "line = quazyilx.map(lambda q:Row(datetime=quazyilx_class(q).datetime.isoformat(), fnard=quazyilx_class(q).fnard, fnok=quazyilx_class(q).fnok,cark=quazyilx_class(q).cark,gnuck=quazyilx_class(q).gnuck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 rows to make sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cark=19, datetime='2000-01-01T00:00:03', fnard=7, fnok=8, gnuck=25),\n",
       " Row(cark=16, datetime='2000-01-01T00:00:08', fnard=14, fnok=19, gnuck=37),\n",
       " Row(cark=12, datetime='2000-01-01T00:00:17', fnard=12, fnok=11, gnuck=8),\n",
       " Row(cark=3, datetime='2000-01-01T00:00:22', fnard=18, fnok=16, gnuck=8),\n",
       " Row(cark=7, datetime='2000-01-01T00:00:32', fnard=7, fnok=16, gnuck=37),\n",
       " Row(cark=3, datetime='2000-01-01T00:00:40', fnard=6, fnok=14, gnuck=30),\n",
       " Row(cark=17, datetime='2000-01-01T00:00:47', fnard=11, fnok=10, gnuck=7),\n",
       " Row(cark=13, datetime='2000-01-01T00:00:55', fnard=9, fnok=14, gnuck=30),\n",
       " Row(cark=7, datetime='2000-01-01T00:00:56', fnard=10, fnok=1, gnuck=6),\n",
       " Row(cark=12, datetime='2000-01-01T00:00:59', fnard=11, fnok=11, gnuck=18)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, convert the quazyilx RDD into a DataFrame `quazyilx_df` using the `spark.createDataFrame` method, register it as the SQL table `quazyilx_tbl` with the method `.createOrReplaceTempView`. You will want to cache the DataFrame so it doesn't get generated every time you run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cark: bigint, datetime: string, fnard: bigint, fnok: bigint, gnuck: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Let's map the relationships to an RDD of rows in order to create a data frame out of it\n",
    "quazyilx_df = spark.createDataFrame(line)\n",
    "quazyilx_df.createOrReplaceTempView(\"quazyilx_tbl\")\n",
    "quazyilx_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create and register the dataframe and table, you will run SQL queries using  `spark.sql()` to calculate the following:\n",
    "\n",
    "1. The number of rows in the dataset\n",
    "1. The number of lines that has -1 for `fnard`, `fnok`, `cark` and `gnuck`.\n",
    "1. The number of lines that have -1 for `fnard` but have `fnok > 5` and `cark > 5`\n",
    "1. The first datetime in the dataset\n",
    "1. The first datetime that has -1 for all of the values\n",
    "1. The last datetime in the dataset\n",
    "1. The last datetime that has a -1 for all of the values\n",
    "\n",
    "Place each query into each of the following  seven cells and run it to get the results. Remember, running the query statement itselft will not give you the results you want. You need to do something else to \"get\" the result.\n",
    "\n",
    "**Note: in development testing, the first query may take approximately 10-15 minutes to run with the cluster configuration for this assignment (1 master, 4 task nodes of m4.xlarge). If you cache() correctly, all subsequent queries should take no more than 5 seconds.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=100000000)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.sql(\"\"\"SELECT count(*) FROM quazyilx_tbl\"\"\")\n",
    "df1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=190)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM quazyilx_tbl\n",
    "WHERE cark = -1 AND fnard = -1 AND fnok = -1 AND gnuck = -1\n",
    "\"\"\")\n",
    "df2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=2114009)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM quazyilx_tbl\n",
    "WHERE cark > 5 AND fnard  = -1 AND fnok > 5\n",
    "\"\"\")\n",
    "df3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=u'2000-01-01T00:00:03')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = spark.sql(\"\"\"\n",
    "SELECT datetime\n",
    "FROM quazyilx_tbl\n",
    "ORDER BY datetime\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "df4.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=u'2000-01-28T03:07:44')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = spark.sql(\"\"\"\n",
    "SELECT datetime\n",
    "FROM quazyilx_tbl\n",
    "WHERE cark = -1 AND fnard = -1 AND fnok = -1 AND gnuck = -1\n",
    "ORDER BY datetime\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "df5.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=u'2017-06-05T18:03:07')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = spark.sql(\"\"\"\n",
    "SELECT datetime\n",
    "FROM quazyilx_tbl\n",
    "ORDER BY datetime DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "df6.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=u'2017-04-21T04:57:10')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = spark.sql(\"\"\"\n",
    "SELECT datetime\n",
    "FROM quazyilx_tbl\n",
    "WHERE cark = -1 AND fnard = -1 AND fnok = -1 AND gnuck = -1\n",
    "ORDER BY datetime DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "df7.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you finish this problem, click on the File -> 'Save and Checkpoint' in the menu bar to make sure that the latest version of the workbook file is saved. Also, before you close this notebook and move on, make sure you disconnect your SparkContext, otherwise you will not be able to re-allocate resources. Remember, you will commit the .ipynb file to the repository for submission (in the master node terminal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
